import asyncio
import io
from typing import get_args

from kokoro import KPipeline
import numpy as np
import soundfile as sf
import torch

from src.tts.provider import TTSProvider
from src.types.kokoro import KokoroVoices


class KokoroProvider(TTSProvider[KokoroVoices]):
    def __init__(
        self,
        lang_code: str = 'a',
    ) -> None:
        self.pipeline = KPipeline(
            repo_id='hexgrad/Kokoro-82M',
            lang_code=lang_code,
        )

    def _synthesize_speech(
        self,
        voice: KokoroVoices,
        text: str,
    ) -> bytes:
        generator = self.pipeline(text, voice=voice)
        audio: torch.FloatTensor
        result: np.ndarray | None = None

        for _, _, audio in generator:  # type: ignore
            result = audio.numpy()

        if result is None:
            raise RuntimeError("No audio generated by Kokoro pipeline.")

        # Audio is 24kHz, mono pcm. We need to convert to mp3 bytes.
        with io.BytesIO() as buffer:
            sf.write(
                buffer,
                result,
                samplerate=24000,
                format='MP3',
                # subtype='PCM_16',
            )

            return buffer.getvalue()

    async def synthesize_speech(
        self,
        voice: KokoroVoices,
        text: str,
        **kwargs,
    ) -> bytes:
        event_loop = asyncio.get_running_loop()

        return await event_loop.run_in_executor(
            None,
            self._synthesize_speech,
            voice,
            text,
        )

    @property
    def can_cache(self) -> bool:
        return True  # Kokoro voices are standard voices and can be cached.

    @property
    def supports_ssml(self) -> bool:
        return False

    @property
    def has_financial_cost(self) -> bool:
        return False  # Kokoro is open-source and free to use.

    @staticmethod
    def voices() -> list[str]:
        return list(get_args(KokoroVoices))
